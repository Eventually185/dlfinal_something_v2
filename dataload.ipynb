{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c069c1f6",
   "metadata": {},
   "source": [
    "\n",
    "# Load dataset and prepare for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e0d893",
   "metadata": {},
   "source": [
    "### pip install datasets opencv-python pillow tqdm \n",
    "### pip install huggingface_hub\n",
    "### pip3 install torch torchvision(cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba79ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\dl-final\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import glob\n",
    "import shutil\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6690ccb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data directory: D:/A-computer files/Deep Learning/Final/data/something_v2\n",
      "output directory: D:/A-computer files/Deep Learning/Final/processed_something_v2\n"
     ]
    }
   ],
   "source": [
    "#  + labels.json \n",
    "DATA_DIR = \"D:/A-computer files/Deep Learning/Final/data/something_v2\"   # dataset directory\n",
    "\n",
    "# processed data directory\n",
    "SAVE_ROOT = \"D:/A-computer files/Deep Learning/Final/processed_something_v2\"\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "\n",
    "print(\"data directory:\", DATA_DIR)\n",
    "print(\"output directory:\", SAVE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44dc5a",
   "metadata": {},
   "source": [
    "### Download the dataset from Hugging Face Hub https://huggingface.co/datasets/HuggingFaceM4/something_something_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6f1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]d:\\Anaconda\\envs\\dl-final\\Lib\\site-packages\\huggingface_hub\\file_download.py:121: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\A-computer files\\Deep Learning\\Final\\data\\something_v2\\datasets--HuggingFaceM4--something_something_v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:01<00:00,  3.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:/A-computer files/Deep Learning/Final/data/something_v2\\\\datasets--HuggingFaceM4--something_something_v2\\\\snapshots\\\\130db220f301e31219875231983a9827c8370aa1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Download the dataset from Hugging Face Hub\n",
    "snapshot_download(\n",
    "    repo_id=\"HuggingFaceM4/something_something_v2\",\n",
    "    repo_type=\"dataset\",\n",
    "    cache_dir=DATA_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844683f",
   "metadata": {},
   "source": [
    "### Download datasets by hand from https://www.qualcomm.com/developer/software/something-something-v-2-dataset/downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cae36d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 2 parts into videos.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Concatenate split tar.gz parts in a cross-platform way using Python.\n",
    "# It reads all parts matching the pattern, sorts them, and writes them into a single output file.\n",
    "\n",
    "parts = sorted(glob.glob(\"20bn-something-something-v2-*.tar.gz\"))\n",
    "if not parts:\n",
    "\traise FileNotFoundError(\"No files found matching pattern: 20bn-something-something-v2-*.tar.gz\")\n",
    "\n",
    "output_path = \"videos.tar.gz\"\n",
    "with open(output_path, \"wb\") as outfile:\n",
    "\tfor part in parts:\n",
    "\t\twith open(part, \"rb\") as infile:\n",
    "\t\t\tshutil.copyfileobj(infile, outfile)\n",
    "\n",
    "print(f\"Combined {len(parts)} parts into {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f629820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure target directory exists (os is already imported in another cell)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Use the already-created output_path (20bn-something-something-v2.tar.gz)\n",
    "with tarfile.open(output_path, \"r:gz\") as tar:\n",
    "\ttar.extractall(path=DATA_DIR)\n",
    "\n",
    "print(f\"Extracted {output_path} to {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e873e037",
   "metadata": {},
   "source": [
    "Remember to change the directory name from 20bn-something-something-v2 to videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e6fb761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 168913 examples [00:00, 250933.63 examples/s]\n",
      "Generating validation split: 24777 examples [00:00, 292625.04 examples/s]\n",
      "Generating test split: 27157 examples [00:00, 1078724.84 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'label', 'template', 'placeholders'],\n",
      "        num_rows: 168913\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'label', 'template', 'placeholders'],\n",
      "        num_rows: 24777\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'label', 'template', 'placeholders'],\n",
      "        num_rows: 27157\n",
      "    })\n",
      "})\n",
      "{'id': '78687', 'label': 'holding potato next to vicks vaporub bottle', 'template': 'Holding [something] next to [something]', 'placeholders': ['potato', 'vicks vaporub bottle']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from the local extracted directory using Hugging Face Datasets\n",
    "dataset = load_dataset(DATA_DIR)\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f495e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_KEYWORDS = {\n",
    "    \"move_object\": [\n",
    "        \"moving\", \n",
    "        \"pushing\", \n",
    "    ],\n",
    "    \"drop_object\": [\n",
    "        \"dropping\", \n",
    "        \"letting something roll down\"\n",
    "    ],\n",
    "    \"cover_object\": [\n",
    "        \"covering\", \n",
    "        \"putting\",\n",
    "    ],\n",
    "    # extra tasks including opening, throwing and catching, pulling...\n",
    "    \"open_object\": [\n",
    "        \"opening\",\n",
    "        \"unfolding\"\n",
    "    ],\n",
    "    \"throw_and_catch_object\": [\n",
    "        \"throwing\", \n",
    "        \"catching\"\n",
    "    ],\n",
    "    \"pull_object\": [\n",
    "        \"pulling\", \n",
    "        \"dragging\"\n",
    "    ],\n",
    "    \"tilting_object\": [\n",
    "        \"tilting\",\n",
    "        \"Tipping\"\n",
    "    ],\n",
    "    \"stacking_object\": [\n",
    "        \"stacking\",\n",
    "        \"Piling\"\n",
    "    ],\n",
    "    \"pouring_object\": [\n",
    "        \"pouring\",\n",
    "    ],\n",
    "    \"rolling_object\": [\n",
    "        \"rolling\",\n",
    "        \"spinning\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "def match_task(label):\n",
    "    label = label.lower()\n",
    "    for task, keys in TASK_KEYWORDS.items():\n",
    "        for k in keys:\n",
    "            if k in label:\n",
    "                return task\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0dd6298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract n_frames frames uniformly from the video\n",
    "def extract_frames(video_path, n_frames=21):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if total < n_frames:\n",
    "        cap.release()\n",
    "        return None\n",
    "    \n",
    "    idxs = np.linspace(0, total - 1, n_frames, dtype=int)\n",
    "    frames = []\n",
    "\n",
    "    for idx in idxs:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            cap.release()\n",
    "            return None\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03475254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 168913/168913 [6:23:47<00:00,  7.34it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Data statistics: {'move_object': 29262, 'drop_object': 4244, 'cover_object': 22918, 'open_object': 3226, 'throw_and_catch_object': 5539, 'pull_object': 5565, 'tilting_object': 1402, 'stacking_object': 952, 'pouring_object': 1488, 'rolling_object': 2817}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sample_count = {k: 0 for k in TASK_KEYWORDS.keys()}\n",
    "\n",
    "def resolve_video_path(item):\n",
    "    # Try common keys that might contain a path\n",
    "    if isinstance(item, dict):\n",
    "        for key in (\"videos\", \"video\", \"video_path\", \"file_path\", \"path\"):\n",
    "            p = item.get(key)\n",
    "            if p:\n",
    "                # some datasets store a list/tuple\n",
    "                if isinstance(p, (list, tuple)) and p:\n",
    "                    return p[0]\n",
    "                return p\n",
    "\n",
    "    # Fall back to constructing path from id and common directories/extensions\n",
    "    vid_id = item.get(\"id\") if isinstance(item, dict) else None\n",
    "    if vid_id is None:\n",
    "        return None\n",
    "\n",
    "    search_dirs = [\n",
    "        os.path.join(DATA_DIR, \"videos\"),\n",
    "        DATA_DIR,\n",
    "    ]\n",
    "    exts = [\".mp4\", \".webm\", \".avi\", \".mov\", \".mkv\", \".mpeg\"]\n",
    "\n",
    "    for d in search_dirs:\n",
    "        if not d:\n",
    "            continue\n",
    "        for ext in exts:\n",
    "            candidate = os.path.join(d, vid_id + ext)\n",
    "            if os.path.exists(candidate):\n",
    "                return candidate\n",
    "\n",
    "    # As a last resort, try a recursive glob search for files containing the id\n",
    "    pattern = os.path.join(DATA_DIR, \"**\", vid_id + \"*\")\n",
    "    matches = glob.glob(pattern, recursive=True)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    return None\n",
    "\n",
    "for item in tqdm(dataset[\"train\"], desc=\"Processing\"):\n",
    "    label = item.get(\"label\", \"\")\n",
    "    task = match_task(label)\n",
    "    if task is None:\n",
    "        continue\n",
    "\n",
    "    # Resolve video path robustly to avoid KeyError\n",
    "    video_path = resolve_video_path(item)\n",
    "    if video_path is None or not os.path.exists(video_path):\n",
    "        # Could not find a corresponding video file for this sample\n",
    "        continue\n",
    "\n",
    "    frames = extract_frames(video_path, n_frames=21)\n",
    "    if frames is None:\n",
    "        continue\n",
    "\n",
    "    # Resize to 128x128 (>= 96x96 is fine)  \n",
    "    frames = [cv2.resize(f, (128, 128)) for f in frames]\n",
    "\n",
    "    # prepare output path\n",
    "    vid_id = str(sample_count[task]).zfill(5)\n",
    "    vid_dir = os.path.join(SAVE_ROOT, task, f\"video_{vid_id}\")\n",
    "    os.makedirs(os.path.join(vid_dir, \"frames\"), exist_ok=True)\n",
    "\n",
    "    # save frames\n",
    "    for i, frame in enumerate(frames):\n",
    "        save_p = os.path.join(vid_dir, \"frames\", f\"{i:04d}.png\")\n",
    "        cv2.imwrite(save_p, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # save metadata\n",
    "    with open(os.path.join(vid_dir, \"meta.json\"), \"w\") as f:\n",
    "        json.dump({\"label\": label}, f, indent=4)\n",
    "\n",
    "    sample_count[task] += 1\n",
    "\n",
    "print(\"Done! Data statistics:\", sample_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6983e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "class HOIDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.items = []\n",
    "        \n",
    "        tasks = os.listdir(root)\n",
    "        for t in tasks:\n",
    "            tdir = os.path.join(root, t)\n",
    "            videos = os.listdir(tdir)\n",
    "            for vid in videos:\n",
    "                vid_dir = os.path.join(tdir, vid)\n",
    "                meta_path = os.path.join(vid_dir, \"meta.json\")\n",
    "                frames_dir = os.path.join(vid_dir, \"frames\")\n",
    "                \n",
    "                frames = sorted(glob.glob(os.path.join(frames_dir, \"*.png\")))\n",
    "                if len(frames) < 21:\n",
    "                    continue\n",
    "                \n",
    "                self.items.append((frames, meta_path))\n",
    "\n",
    "        self.tf = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames, meta_path = self.items[idx]\n",
    "\n",
    "        with open(meta_path, \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "        text = meta[\"label\"]\n",
    "\n",
    "        input_frames = [self.tf(Image.open(frames[i])) for i in range(20)]\n",
    "        input_frames = torch.stack(input_frames)\n",
    "\n",
    "        target = self.tf(Image.open(frames[20]))\n",
    "\n",
    "        return {\n",
    "            \"input_frames\": input_frames,\n",
    "            \"text\": text,\n",
    "            \"target_frame\": target\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ed52b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 20, 3, 128, 128])\n",
      "torch.Size([4, 3, 128, 128])\n",
      "['dropping razor behind adhesive tape', 'moving remote control towards the camera', 'pushing marking pen so that it slightly moves', 'putting one clip']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ds = HOIDataset(SAVE_ROOT)\n",
    "dl = DataLoader(ds, batch_size=4, shuffle=True)\n",
    "\n",
    "batch = next(iter(dl))\n",
    "print(batch[\"input_frames\"].shape)   # [4, 20, 3, 128, 128]\n",
    "print(batch[\"target_frame\"].shape)   # [4, 3, 128, 128]\n",
    "print(batch[\"text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
